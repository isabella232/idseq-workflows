# idseq-workflows tests

## WDL workflow & task tests

The [pytest](https://docs.pytest.org/en/stable/) suite for each workflow has two stages. First, the workflow is run end-to-end on one or more test inputs (using viral reference databases) and its outputs sanity-checked. Second, task-level unit tests to exercise error paths or corner cases that aren't otherwise covered by the workflow tests. The workflow tests are triggered by session-scoped pytest fixtures which are then used by the task-level cases. This two-stage structure allows the task unit tests to reuse intermediate inputs/outputs generated by the workflow tests, minimizing the boilerplate needed to express them.

For example, `short-read-mngs/conftest.py` has the workflow test fixture for a small synthetic FASTQ pair ("bench3") to run using viral reference databases. Then, `short-read-mngs/host_filter/test_RunValidateInput.py` has unit tests for the RunValidateInput task to verify that it produces expected error messages, given the bench3 workflow inputs modified to pass an invalid FASTQ.

To run the short-read-mngs test suite locally, first enable the miniwdl download cache in (to handle the ~6GB viral reference databases):

```bash
export MINIWDL__DOWNLOAD_CACHE__PUT=true
export MINIWDL__DOWNLOAD_CACHE__GET=true
export MINIWDL__DOWNLOAD_CACHE__DIR=/tmp/miniwdl_download_cache
```

Then, build the docker image from the current code revision and start the tests:

```bash
cd idseq-workflows
docker build -t idseq-short-read-mngs short-read-mngs
make test-short-read-mngs
```

## Automatic benchmarking suite for short-read-mngs

See [short-read-mngs/auto_benchmark/README.md](short-read-mngs/auto_benchmark/)
